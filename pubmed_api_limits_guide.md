# PubMed API é™åˆ¶å’Œæœ€ä½³å®è·µæŒ‡å—

## ğŸ“‹ APIé™åˆ¶æ¦‚è¿°

### 1. è¯·æ±‚é¢‘ç‡é™åˆ¶
- **æœç´¢æ¥å£**: æ¯ç§’æœ€å¤š3ä¸ªè¯·æ±‚
- **è·å–è¯¦æƒ…æ¥å£**: æ¯ç§’æœ€å¤š3ä¸ªè¯·æ±‚
- **æ‰¹é‡è·å–**: å»ºè®®æ¯æ‰¹æ¬¡ä¹‹é—´é—´éš”0.3-0.5ç§’

### 2. æ‰¹é‡å¤§å°é™åˆ¶
- **å•æ¬¡æœç´¢**: æœ€å¤šè¿”å›10,000ä¸ªç»“æœ
- **å•æ¬¡è·å–è¯¦æƒ…**: å»ºè®®ä¸è¶…è¿‡100-200ä¸ªPMID
- **URLé•¿åº¦é™åˆ¶**: GETè¯·æ±‚URLä¸èƒ½è¶…è¿‡2048å­—ç¬¦

### 3. ä½¿ç”¨è¦æ±‚
- **å¿…é¡»æä¾›é‚®ç®±**: ç”¨äºèº«ä»½è¯†åˆ«å’Œé—®é¢˜è”ç³»
- **ç”¨æˆ·ä»£ç†**: å»ºè®®æä¾›åº”ç”¨ç¨‹åºåç§°
- **åˆç†ä½¿ç”¨æ—¶é—´**: é¿å…åœ¨é«˜å³°æ—¶æ®µå¤§é‡è¯·æ±‚

### 4. æ•°æ®è®¿é—®é™åˆ¶
- **æ¯æ—¥æ€»é‡**: æ²¡æœ‰æ˜ç¡®é™åˆ¶ï¼Œä½†å»ºè®®åˆç†æ§åˆ¶
- **å¹¶å‘è¿æ¥**: å»ºè®®å•çº¿ç¨‹æˆ–å°‘é‡å¹¶å‘
- **é‡è¯•æœºåˆ¶**: å¤±è´¥åå»ºè®®ç­‰å¾…1-3ç§’å†é‡è¯•

## ğŸš¨ å¸¸è§é™åˆ¶é”™è¯¯

### 429 Too Many Requests
```xml
<Error>
    <Code>429</Code>
    <Message>Too Many Requests</Message>
    <Details>Rate limit exceeded</Details>
</Error>
```

### 403 Forbidden
```xml
<Error>
    <Code>403</Code>
    <Message>Forbidden</Message>
    <Details>API key required or IP blocked</Details>
</Error>
```

### 500 Internal Error
```xml
<Error>
    <Code>500</Code>
    <Message>Internal Server Error</Message>
</Error>
```

## ğŸ’¡ æœ€ä½³å®è·µå»ºè®®

### 1. è¯·æ±‚é—´éš”æ§åˆ¶
```python
import time

# æœç´¢è¯·æ±‚é—´éš”
time.sleep(0.4)  # æ¨è0.3-0.5ç§’

# è¯¦æƒ…è·å–é—´éš”  
time.sleep(0.5)  # æ¨è0.5-1ç§’

# é”™è¯¯é‡è¯•é—´éš”
time.sleep(3)    # å¤±è´¥åç­‰å¾…3ç§’
```

### 2. æ‰¹é‡å¤„ç†ä¼˜åŒ–
```python
# æ¨èæ‰¹é‡å¤§å°
SEARCH_BATCH_SIZE = 100   # æœç´¢æ‰¹æ¬¡
FETCH_BATCH_SIZE = 50     # è·å–è¯¦æƒ…æ‰¹æ¬¡
MAX_TOTAL_RESULTS = 10000  # æ€»ç»“æœé™åˆ¶
```

### 3. é”™è¯¯å¤„ç†å’Œé‡è¯•
```python
import requests
from time import sleep

def safe_request(url, params, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = requests.get(url, params=params, timeout=30)
            if response.status_code == 200:
                return response
            elif response.status_code == 429:
                sleep_time = (attempt + 1) * 2  # æŒ‡æ•°é€€é¿
                print(f"Rate limit hit, waiting {sleep_time} seconds...")
                sleep(sleep_time)
            else:
                print(f"Error {response.status_code}: {response.text}")
                return None
        except requests.exceptions.RequestException as e:
            print(f"Request failed: {e}")
            if attempt < max_retries - 1:
                sleep(2)
            else:
                return None
    return None
```

### 4. è¯·æ±‚å¤´è®¾ç½®
```python
HEADERS = {
    'User-Agent': 'NEJM-Article-Scraper/1.0 (your_email@example.com)',
    'Accept': 'application/json',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive'
}
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. æ¸è¿›å¼çˆ¬å–
```python
def progressive_crawl(target_count=1000):
    """æ¸è¿›å¼çˆ¬å–ï¼Œé¿å…ä¸€æ¬¡æ€§å¤§é‡è¯·æ±‚"""
    batch_size = 100
    total_fetched = 0
    
    while total_fetched < target_count:
        # è·å–ä¸€æ‰¹æ–‡ç« 
        batch_pmids = fetch_batch(total_fetched, batch_size)
        
        if not batch_pmids:
            break
            
        # å¤„ç†è¿™æ‰¹æ–‡ç« 
        process_batch(batch_pmids)
        
        total_fetched += len(batch_pmids)
        print(f"Progress: {total_fetched}/{target_count}")
        
        # æ‰¹æ¬¡é—´ä¼‘æ¯
        if total_fetched < target_count:
            sleep(1)
```

### 2. æ™ºèƒ½ç¼“å­˜æœºåˆ¶
```python
import json
import os

class ArticleCache:
    def __init__(self, cache_dir="cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def get_cached_article(self, pmid):
        """è·å–ç¼“å­˜çš„æ–‡ç« """
        cache_file = os.path.join(self.cache_dir, f"{pmid}.json")
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None
    
    def cache_article(self, pmid, article_data):
        """ç¼“å­˜æ–‡ç« æ•°æ®"""
        cache_file = os.path.join(self.cache_dir, f"{pmid}.json")
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(article_data, f, ensure_ascii=False, indent=2)
```

### 3. æ–­ç‚¹ç»­çˆ¬åŠŸèƒ½
```python
def crawl_with_checkpoint(start_pmids, checkpoint_file="crawl_checkpoint.json"):
    """æ”¯æŒæ–­ç‚¹ç»­çˆ¬"""
    # åŠ è½½æ£€æŸ¥ç‚¹
    processed_pmids = set()
    if os.path.exists(checkpoint_file):
        with open(checkpoint_file, 'r') as f:
            checkpoint = json.load(f)
            processed_pmids = set(checkpoint.get('processed_pmids', []))
            print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œå·²å¤„ç† {len(processed_pmids)} ç¯‡æ–‡ç« ")
    
    # è¿‡æ»¤æœªå¤„ç†çš„PMID
    remaining_pmids = [pmid for pmid in start_pmids if pmid not in processed_pmids]
    
    results = []
    for i, pmid in enumerate(remaining_pmids):
        try:
            # è·å–æ–‡ç« è¯¦æƒ…
            article = fetch_article_detail(pmid)
            if article:
                results.append(article)
            
            # æ›´æ–°æ£€æŸ¥ç‚¹
            processed_pmids.add(pmid)
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (i + 1) % 50 == 0:
                save_checkpoint(checkpoint_file, processed_pmids, results)
                print(f"ä¿å­˜æ£€æŸ¥ç‚¹: {i+1}/{len(remaining_pmids)}")
                
        except Exception as e:
            print(f"å¤„ç†PMID {pmid} å¤±è´¥: {e}")
            continue
    
    # æœ€ç»ˆä¿å­˜
    save_checkpoint(checkpoint_file, processed_pmids, results)
    return results
```

## ğŸ¯ æ¨èçš„çˆ¬å–ç­–ç•¥

### 1. åˆ†æ—¶æ®µçˆ¬å–
```python
def smart_crawl_schedule():
    """æ™ºèƒ½çˆ¬å–æ—¶é—´å®‰æ’"""
    import datetime
    
    now = datetime.datetime.now()
    hour = now.hour
    
    # é¿å¼€é«˜å³°æ—¶æ®µ (9-17ç‚¹)
    if 9 <= hour <= 17:
        print("é«˜å³°æ—¶æ®µï¼Œå»¶é•¿ç­‰å¾…æ—¶é—´")
        sleep_time = 2.0
    else:
        print("éé«˜å³°æ—¶æ®µï¼Œæ­£å¸¸é€Ÿåº¦")
        sleep_time = 0.5
    
    return sleep_time
```

### 2. ä¼˜å…ˆçº§é˜Ÿåˆ—
```python
from queue import PriorityQueue

def priority_based_crawl(pmids_with_priority):
    """åŸºäºä¼˜å…ˆçº§çš„çˆ¬å–"""
    pq = PriorityQueue()
    
    # æ·»åŠ PMIDåˆ°ä¼˜å…ˆçº§é˜Ÿåˆ—
    for priority, pmid in pmids_with_priority:
        pq.put((priority, pmid))
    
    results = []
    while not pq.empty():
        priority, pmid = pq.get()
        
        try:
            article = fetch_article_detail(pmid)
            if article:
                results.append((priority, article))
                print(f"é«˜ä¼˜å…ˆçº§æ–‡ç« è·å–æˆåŠŸ: PMID {pmid}")
        except Exception as e:
            print(f"ä¼˜å…ˆçº§ {priority} PMID {pmid} å¤±è´¥: {e}")
        
        sleep(0.5)  # æ§åˆ¶é¢‘ç‡
    
    return results
```

## ğŸ“ˆ ç›‘æ§å’Œç»Ÿè®¡

### 1. çˆ¬å–ç»Ÿè®¡
```python
class CrawlStats:
    def __init__(self):
        self.start_time = time.time()
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.rate_limit_hits = 0
    
    def log_request(self, success, rate_limited=False):
        self.total_requests += 1
        if success:
            self.successful_requests += 1
        else:
            self.failed_requests += 1
        
        if rate_limited:
            self.rate_limit_hits += 1
    
    def get_stats(self):
        elapsed_time = time.time() - self.start_time
        success_rate = (self.successful_requests / max(self.total_requests, 1)) * 100
        
        return {
            'total_requests': self.total_requests,
            'successful_requests': self.successful_requests,
            'failed_requests': self.failed_requests,
            'rate_limit_hits': self.rate_limit_hits,
            'success_rate': f"{success_rate:.1f}%",
            'elapsed_time': f"{elapsed_time:.1f}ç§’",
            'requests_per_second': self.total_requests / max(elapsed_time, 1)
        }
```

### 2. å®æ—¶ç›‘æ§
```python
def monitor_crawl_progress(stats, check_interval=60):
    """å®æ—¶ç›‘æ§çˆ¬å–è¿›åº¦"""
    while True:
        current_stats = stats.get_stats()
        print(f"\n{'='*50}")
        print(f"ğŸ“Š çˆ¬å–ç»Ÿè®¡ (æ¯{check_interval}ç§’æ›´æ–°)")
        print(f"æ€»è¯·æ±‚æ•°: {current_stats['total_requests']}")
        print(f"æˆåŠŸ: {current_stats['successful_requests']}")
        print(f"å¤±è´¥: {current_stats['failed_requests']}")
        print(f"æˆåŠŸç‡: {current_stats['success_rate']}")
        print(f"é€Ÿç‡é™åˆ¶: {current_stats['rate_limit_hits']}")
        print(f"è¯·æ±‚é€Ÿåº¦: {current_stats['requests_per_second']:.2f}/ç§’")
        print(f"è¿è¡Œæ—¶é—´: {current_stats['elapsed_time']}")
        print(f"{'='*50}\n")
        
        time.sleep(check_interval)
```

## ğŸ”§ æ•…éšœæ’é™¤

### 1. è¿æ¥è¶…æ—¶
```python
# å¢åŠ è¶…æ—¶æ—¶é—´
response = requests.get(url, params=params, timeout=60)

# ä½¿ç”¨ä¼šè¯ä¿æŒè¿æ¥
session = requests.Session()
session.headers.update({'Connection': 'keep-alive'})
```

### 2. å†…å­˜ä¼˜åŒ–
```python
# æµå¼å¤„ç†å¤§æ–‡ä»¶
def process_large_dataset(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            # é€è¡Œå¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡º
            process_line(line.strip())
```

### 3. ä»£ç†è®¾ç½®
```python
# ä½¿ç”¨ä»£ç†é¿å…IPé™åˆ¶
proxies = {
    'http': 'http://proxy.example.com:8080',
    'https': 'https://proxy.example.com:8080'
}

response = requests.get(url, params=params, proxies=proxies)
```

## ğŸ“š å®˜æ–¹æ–‡æ¡£å‚è€ƒ

- **PubMed E-utilities**: https://www.ncbi.nlm.nih.gov/books/NBK25501/
- **Rate Limiting Guidelines**: https://www.ncbi.nlm.nih.gov/home/about/policies/
- **Best Practices**: https://www.ncbi.nlm.nih.gov/pmc/tools/developers/

è®°ä½ï¼š**åˆç†çˆ¬å–ï¼Œå°Šé‡æœåŠ¡å™¨èµ„æºï¼Œé¿å…å½±å“å…¶ä»–ç”¨æˆ·ï¼**