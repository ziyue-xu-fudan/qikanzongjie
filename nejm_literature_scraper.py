#!/usr/bin/env python3
"""
ä¸“ä¸šç‰ˆNEJMæ–‡çŒ®çˆ¬å–è„šæœ¬
ä¸“é—¨é’ˆå¯¹æ–°è‹±æ ¼å…°åŒ»å­¦æ‚å¿—çš„æ–‡çŒ®çˆ¬å–ï¼ŒåŒ…å«å®Œæ•´çš„æ–‡ç« ç±»å‹è¯†åˆ«å’Œå¢é‡æ›´æ–°åŠŸèƒ½
"""

import requests
import json
import pandas as pd
from datetime import datetime, timedelta
import time
import os
import re
from typing import List, Dict, Optional, Set
from pathlib import Path

class NEJMLiteratureScraper:
    def __init__(self, email: str = "nejm.scraper@example.com"):
        """
        åˆå§‹åŒ–NEJMä¸“ä¸šçˆ¬å–å™¨
        
        Args:
            email: ç”¨äºPubMed APIçš„é‚®ç®±åœ°å€
        """
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        self.search_url = self.base_url + "esearch.fcgi"
        self.fetch_url = self.base_url + "efetch.fcgi"
        self.journal_name = "N Engl J Med"
        self.email = email
        
        # NEJMç‰¹å®šçš„æ–‡ç« ç±»å‹è¯†åˆ«æ¨¡å¼
        self.nejm_patterns = {
            'original_article': [
                r'original article',
                r'original research',
                r'clinical research',
                r'research article',
                r'clinical trial',
                r'observational study',
                r'randomized.*trial',
                r'prospective.*study',
                r'retrospective.*study'
            ],
            'correspondence': [
                r'correspondence',
                r'letter.*to.*editor',
                r'reply.*to',
                r'response.*to',
                r'letter.*regarding',
                r're.*:',  # å›å¤ç±»æ ‡é¢˜
                r'^[^.]*\.\s*reply\.',  # ä»¥"reply."ç»“å°¾çš„æ ‡é¢˜
                r'^[^.]*\.\s*response\.'  # ä»¥"response."ç»“å°¾çš„æ ‡é¢˜
            ],
            'review': [
                r'review.*article',
                r'systematic.*review',
                r'meta.*analysis',
                r'narrative.*review',
                r'clinical.*review'
            ],
            'case_report': [
                r'case.*report',
                r'case.*series',
                r'clinical.*case'
            ],
            'editorial': [
                r'editorial',
                r'editor\'s.*note',
                r'perspective',
                r'viewpoint'
            ]
        }
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': f'NEJM-Literature-Scraper/1.0 ({email})',
            'Accept': 'application/json',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        }
        
        # ç¼“å­˜å’ŒçŠ¶æ€ç®¡ç†
        self.cache_dir = Path("nejm_cache")
        self.cache_dir.mkdir(exist_ok=True)
        self.processed_pmids: Set[str] = set()
        self.stats = {
            'total_searched': 0,
            'total_fetched': 0,
            'by_type': {},
            'by_year': {},
            'errors': 0
        }

    def search_nejm_literature(self, start_date: str, end_date: str, max_results: int = 5000) -> List[str]:
        """
        æœç´¢NEJMæ–‡çŒ®ï¼ˆä¸“ä¸šç‰ˆï¼‰
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY/MM/DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY/MM/DD)
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°
            
        Returns:
            PMIDåˆ—è¡¨
        """
        print(f"ğŸ” æœç´¢NEJMæ–‡çŒ®: {start_date} - {end_date}")
        
        # æ„å»ºä¸“ä¸šæœç´¢æŸ¥è¯¢
        base_query = f'"{self.journal_name}"[Journal] AND ("{start_date}"[Date - Publication] : "{end_date}"[Date - Publication])'
        
        all_pmids = []
        retmax = 200  # æ¯æ‰¹æ¬¡çš„æ•°é‡
        retstart = 0
        
        while retstart < max_results:
            params = {
                'db': 'pubmed',
                'term': base_query,
                'retmode': 'json',
                'retmax': min(retmax, max_results - retstart),
                'retstart': retstart,
                'email': self.email
            }
            
            try:
                response = requests.get(self.search_url, params=params, headers=self.headers, timeout=30)
                response.raise_for_status()
                
                data = response.json()
                esearchresult = data.get('esearchresult', {})
                
                batch_pmids = esearchresult.get('idlist', [])
                if not batch_pmids:
                    break
                    
                all_pmids.extend(batch_pmids)
                total_found = int(esearchresult.get('count', 0))
                
                print(f"ğŸ“„ å·²è·å–: {len(all_pmids)}/{total_found} ç¯‡æ–‡çŒ®")
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é™åˆ¶
                if len(all_pmids) >= max_results:
                    all_pmids = all_pmids[:max_results]
                    break
                
                retstart += len(batch_pmids)
                time.sleep(0.5)  # æ›´é•¿çš„å»¶è¿Ÿï¼Œå°Šé‡æœåŠ¡å™¨
                
            except requests.exceptions.RequestException as e:
                print(f"âŒ æœç´¢è¯·æ±‚å¤±è´¥: {e}")
                self.stats['errors'] += 1
                time.sleep(2)  # é”™è¯¯åç­‰å¾…æ›´é•¿æ—¶é—´
                continue
                
        self.stats['total_searched'] = len(all_pmids)
        print(f"âœ… æœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_pmids)} ç¯‡NEJMæ–‡çŒ®")
        return all_pmids

    def fetch_literature_details(self, pmids: List[str]) -> List[Dict]:
        """
        è·å–æ–‡çŒ®è¯¦ç»†ä¿¡æ¯ï¼ˆä¸“ä¸šç‰ˆï¼‰
        """
        if not pmids:
            return []
            
        print(f"ğŸ“– å¼€å§‹è·å–æ–‡çŒ®è¯¦æƒ…: {len(pmids)} ç¯‡")
        articles = []
        batch_size = 50  # è¾ƒå°çš„æ‰¹æ¬¡ï¼Œæé«˜ç¨³å®šæ€§
        
        for i in range(0, len(pmids), batch_size):
            batch_pmids = pmids[i:i+batch_size]
            print(f"ğŸ“š å¤„ç†æ‰¹æ¬¡: {i+1}-{min(i+batch_size, len(pmids))}/{len(pmids)}")
            
            params = {
                'db': 'pubmed',
                'id': ','.join(batch_pmids),
                'retmode': 'xml',
                'email': self.email
            }
            
            try:
                response = requests.get(self.fetch_url, params=params, headers=self.headers, timeout=60)
                response.raise_for_status()
                
                batch_articles = self.parse_nejm_xml(response.text)
                articles.extend(batch_articles)
                
                print(f"âœ… æ‰¹æ¬¡å®Œæˆ: è·å–åˆ° {len(batch_articles)} ç¯‡è¯¦ç»†ä¿¡æ¯")
                time.sleep(1)  # æ‰¹æ¬¡é—´è¾ƒé•¿å»¶è¿Ÿ
                
            except Exception as e:
                print(f"âŒ è·å–æ‰¹æ¬¡è¯¦æƒ…å¤±è´¥: {e}")
                self.stats['errors'] += 1
                time.sleep(3)  # é”™è¯¯åæ›´é•¿ç­‰å¾…
                continue
        
        self.stats['total_fetched'] = len(articles)
        return articles

    def parse_nejm_xml(self, xml_content: str) -> List[Dict]:
        """
        ä¸“ä¸šç‰ˆNEJM XMLè§£æ
        """
        articles = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æXMLï¼ˆæ›´ç¨³å®šï¼‰
        article_blocks = re.findall(r'<PubmedArticle>(.*?)</PubmedArticle>', xml_content, re.DOTALL)
        
        for block in article_blocks:
            try:
                article = self.extract_nejm_article_info(block)
                if article and article.get('title'):
                    articles.append(article)
            except Exception as e:
                print(f"âš ï¸  è§£æå•ç¯‡æ–‡çŒ®å¤±è´¥: {e}")
                continue
                
        return articles

    def extract_nejm_article_info(self, xml_block: str) -> Optional[Dict]:
        """
        æå–NEJMæ–‡ç« çš„ä¸“ä¸šä¿¡æ¯
        """
        article = {}
        
        # PMID
        pmid_match = re.search(r'<PMID[^>]*>(\d+)</PMID>', xml_block)
        article['pmid'] = pmid_match.group(1) if pmid_match else ''
        
        if not article['pmid']:
            return None
        
        # æ ‡é¢˜ï¼ˆæ›´ç²¾ç¡®æå–ï¼‰
        title_match = re.search(r'<ArticleTitle[^>]*>(.*?)</ArticleTitle>', xml_block, re.DOTALL)
        if title_match:
            title = self.clean_xml_text(title_match.group(1))
            article['title'] = title
        else:
            article['title'] = ''
        
        # ä½œè€…ï¼ˆæ›´å®Œæ•´æå–ï¼‰
        authors = []
        author_blocks = re.findall(r'<Author[^>]*>(.*?)</Author>', xml_block, re.DOTALL)
        
        for author_block in author_blocks:
            lastname = self.extract_xml_field(author_block, 'LastName')
            forename = self.extract_xml_field(author_block, 'ForeName')
            initials = self.extract_xml_field(author_block, 'Initials')
            
            if lastname:
                if forename:
                    authors.append(f"{lastname} {forename}")
                elif initials:
                    authors.append(f"{lastname} {initials}")
                else:
                    authors.append(lastname)
        
        article['authors'] = ', '.join(authors[:8])  # é™åˆ¶å‰8ä¸ªä½œè€…
        article['author_count'] = len(authors)
        
        # æœŸåˆŠä¿¡æ¯
        journal_title = self.extract_xml_field(xml_block, 'Title')
        article['journal'] = journal_title if journal_title else self.journal_name
        
        # å‘è¡¨æ—¥æœŸï¼ˆæ›´è¯¦ç»†ï¼‰
        year = self.extract_xml_field(xml_block, 'Year')
        month = self.extract_xml_field(xml_block, 'Month')
        day = self.extract_xml_field(xml_block, 'Day')
        
        article['pub_year'] = year
        article['pub_month'] = month
        article['pub_day'] = day
        article['pub_date'] = self.format_date(year, month, day)
        
        # DOI
        doi_match = re.search(r'<ELocationID[^>]*EIdType="doi"[^>]*>(.*?)</ELocationID>', xml_block)
        article['doi'] = doi_match.group(1) if doi_match else ''
        
        # æ‘˜è¦ï¼ˆæ›´å®Œæ•´æå–ï¼‰
        abstract_blocks = re.findall(r'<AbstractText[^>]*>(.*?)</AbstractText>', xml_block, re.DOTALL)
        abstract_parts = []
        
        for abstract_block in abstract_blocks:
            abstract_text = self.clean_xml_text(abstract_block)
            if abstract_text and len(abstract_text) > 10:  # è¿‡æ»¤è¿‡çŸ­çš„æ‘˜è¦
                abstract_parts.append(abstract_text)
        
        article['abstract'] = ' '.join(abstract_parts) if abstract_parts else ''
        article['abstract_length'] = len(article['abstract'])
        
        # æ–‡ç« ç±»å‹ï¼ˆä¸“ä¸šè¯†åˆ«ï¼‰
        article['article_type'] = self.classify_nejm_article_type(xml_block, article['title'], article['abstract'])
        
        # å…³é”®è¯
        keywords = []
        keyword_matches = re.findall(r'<Keyword[^>]*>(.*?)</Keyword>', xml_block)
        for keyword in keyword_matches:
            clean_keyword = self.clean_xml_text(keyword)
            if clean_keyword:
                keywords.append(clean_keyword)
        
        article['keywords'] = ', '.join(keywords[:10])  # é™åˆ¶å‰10ä¸ªå…³é”®è¯
        article['keyword_count'] = len(keywords)
        
        # é¢å¤–ä¿¡æ¯
        article['language'] = self.extract_xml_field(xml_block, 'Language')
        article['publication_types'] = self.extract_publication_types(xml_block)
        
        # æŠ“å–æ—¶é—´æˆ³
        article['scraped_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        article['scraped_timestamp'] = int(time.time())
        
        return article

    def extract_xml_field(self, xml_block: str, field_name: str) -> str:
        """æå–XMLå­—æ®µ"""
        pattern = rf'<{field_name}[^>]*>(.*?)</{field_name}>'
        match = re.search(pattern, xml_block, re.DOTALL)
        return self.clean_xml_text(match.group(1)) if match else ''

    def extract_publication_types(self, xml_block: str) -> str:
        """æå–å‘è¡¨ç±»å‹"""
        types = []
        type_matches = re.findall(r'<PublicationType[^>]*>(.*?)</PublicationType>', xml_block)
        
        for type_match in type_matches:
            clean_type = self.clean_xml_text(type_match)
            if clean_type:
                types.append(clean_type)
        
        return ', '.join(types[:5])  # é™åˆ¶å‰5ä¸ªç±»å‹

    def classify_nejm_article_type(self, xml_block: str, title: str, abstract: str) -> str:
        """
        ä¸“ä¸šNEJMæ–‡ç« ç±»å‹åˆ†ç±»
        """
        content = (xml_block + " " + title + " " + abstract).lower()
        
        # æ£€æŸ¥Correspondenceï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        for pattern in self.nejm_patterns['correspondence']:
            if re.search(pattern, content, re.IGNORECASE):
                return 'Correspondence'
        
        # æ£€æŸ¥Original Article
        for pattern in self.nejm_patterns['original_article']:
            if re.search(pattern, content, re.IGNORECASE):
                return 'Original Article'
        
        # æ£€æŸ¥Review
        for pattern in self.nejm_patterns['review']:
            if re.search(pattern, content, re.IGNORECASE):
                return 'Review'
        
        # æ£€æŸ¥Case Report
        for pattern in self.nejm_patterns['case_report']:
            if re.search(pattern, content, re.IGNORECASE):
                return 'Case Report'
        
        # æ£€æŸ¥Editorial
        for pattern in self.nejm_patterns['editorial']:
            if re.search(pattern, content, re.IGNORECASE):
                return 'Editorial'
        
        # åŸºäºæ ‡é¢˜ç‰¹å¾çš„é¢å¤–æ£€æŸ¥
        title_lower = title.lower()
        
        # Correspondenceçš„æ ‡é¢˜ç‰¹å¾
        if any(word in title_lower for word in ['reply', 'response', 'letter', 'correspondence']):
            return 'Correspondence'
        
        # Original Articleçš„æ ‡é¢˜ç‰¹å¾
        if any(word in title_lower for word in ['trial', 'study', 'effect', 'efficacy', 'safety', 'outcome']):
            if len(abstract) > 500:  # Original Articleé€šå¸¸æœ‰è¾ƒé•¿çš„æ‘˜è¦
                return 'Original Article'
        
        # é»˜è®¤åˆ†ç±»
        return 'Other'

    def clean_xml_text(self, text: str) -> str:
        """æ¸…ç†XMLæ–‡æœ¬"""
        if not text:
            return ''
        
        # ç§»é™¤XMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # æ›¿æ¢HTMLå®ä½“
        text = text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
        text = text.replace('&quot;', '"').replace('&apos;', "'")
        
        # ç§»é™¤å¤šä½™ç©ºç™½å’Œç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text

    def format_date(self, year: str, month: str, day: str) -> str:
        """æ ¼å¼åŒ–æ—¥æœŸ"""
        if not year:
            return ''
        
        date_parts = [year]
        if month:
            date_parts.append(month)
        if day:
            date_parts.append(day)
        
        return ' '.join(date_parts)

    def filter_target_articles(self, articles: List[Dict]) -> List[Dict]:
        """
        ç­›é€‰ç›®æ ‡æ–‡ç« ç±»å‹ï¼ˆOriginal Articleå’ŒCorrespondenceï¼‰
        """
        target_types = ['Original Article', 'Correspondence']
        filtered = []
        
        for article in articles:
            article_type = article.get('article_type', 'Other')
            if article_type in target_types:
                filtered.append(article)
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats['by_type'][article_type] = self.stats['by_type'].get(article_type, 0) + 1
                
                # æŒ‰å¹´ä»½ç»Ÿè®¡
                year = article.get('pub_year', 'Unknown')
                if year and year.isdigit():
                    self.stats['by_year'][year] = self.stats['by_year'].get(year, 0) + 1
        
        return filtered

    def save_literature_data(self, articles: List[Dict], base_filename: str = None) -> Dict[str, str]:
        """
        ä¿å­˜æ–‡çŒ®æ•°æ®åˆ°å¤šç§æ ¼å¼
        """
        if not base_filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_filename = f"nejm_literature_{timestamp}"
        
        saved_files = {}
        
        try:
            # CSVæ ¼å¼
            csv_file = f"{base_filename}.csv"
            df = pd.DataFrame(articles)
            
            # ç¡®ä¿åˆ—é¡ºåºåˆç†
            columns = ['pmid', 'title', 'authors', 'author_count', 'journal', 'pub_year', 
                      'pub_month', 'pub_day', 'pub_date', 'doi', 'article_type', 
                      'abstract', 'abstract_length', 'keywords', 'keyword_count',
                      'language', 'publication_types', 'scraped_date']
            
            # åªä¿ç•™å­˜åœ¨çš„åˆ—
            existing_columns = [col for col in columns if col in df.columns]
            df = df[existing_columns]
            
            df.to_csv(csv_file, index=False, encoding='utf-8')
            saved_files['csv'] = csv_file
            
            # JSONæ ¼å¼ï¼ˆåŒ…å«æ‰€æœ‰æ•°æ®ï¼‰
            json_file = f"{base_filename}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            saved_files['json'] = json_file
            
            # Excelæ ¼å¼
            excel_file = f"{base_filename}.xlsx"
            df.to_excel(excel_file, index=False)
            saved_files['excel'] = excel_file
            
            # Markdownæ ¼å¼ï¼ˆæ‘˜è¦ç‰ˆï¼‰
            markdown_file = f"{base_filename}_summary.md"
            self.create_markdown_summary(articles, markdown_file)
            saved_files['markdown'] = markdown_file
            
            # ç»Ÿè®¡æŠ¥å‘Š
            stats_file = f"{base_filename}_stats.json"
            self.save_statistics(stats_file)
            saved_files['stats'] = stats_file
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            
        return saved_files

    def create_markdown_summary(self, articles: List[Dict], filename: str):
        """åˆ›å»ºMarkdownæ ¼å¼çš„æ‘˜è¦æŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# NEJMæ–‡çŒ®æ‘˜è¦æŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"æ€»æ–‡çŒ®æ•°: {len(articles)}\n\n")
            
            # æŒ‰ç±»å‹åˆ†ç»„
            by_type = {}
            for article in articles:
                article_type = article.get('article_type', 'Unknown')
                if article_type not in by_type:
                    by_type[article_type] = []
                by_type[article_type].append(article)
            
            for article_type, type_articles in by_type.items():
                f.write(f"## {article_type} ({len(type_articles)}ç¯‡)\n\n")
                
                # æ˜¾ç¤ºå‰10ç¯‡
                for i, article in enumerate(type_articles[:10], 1):
                    title = article.get('title', 'æ— æ ‡é¢˜')
                    authors = article.get('authors', 'æœªçŸ¥ä½œè€…')
                    pub_date = article.get('pub_date', 'æœªçŸ¥æ—¥æœŸ')
                    pmid = article.get('pmid', '')
                    
                    f.write(f"### {i}. {title}\n")
                    f.write(f"- **ä½œè€…**: {authors}\n")
                    f.write(f"- **å‘è¡¨æ—¥æœŸ**: {pub_date}\n")
                    f.write(f"- **PMID**: {pmid}\n")
                    
                    abstract = article.get('abstract', '')
                    if abstract:
                        abstract_summary = abstract[:300] + '...' if len(abstract) > 300 else abstract
                        f.write(f"- **æ‘˜è¦**: {abstract_summary}\n")
                    
                    f.write("\n")
                
                if len(type_articles) > 10:
                    f.write(f"*... è¿˜æœ‰ {len(type_articles) - 10} ç¯‡æ–‡çŒ®*\n\n")

    def save_statistics(self, filename: str):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats_data = {
            'summary': {
                'total_searched': self.stats['total_searched'],
                'total_fetched': self.stats['total_fetched'],
                'errors': self.stats['errors'],
                'success_rate': f"{(self.stats['total_fetched'] / max(self.stats['total_searched'], 1) * 100):.1f}%"
            },
            'by_type': self.stats['by_type'],
            'by_year': dict(sorted(self.stats['by_year'].items())),
            'generated_at': datetime.now().isoformat()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(stats_data, f, ensure_ascii=False, indent=2)

    def display_statistics(self):
        """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ğŸ“Š NEJMæ–‡çŒ®çˆ¬å–ç»Ÿè®¡æŠ¥å‘Š")
        print("="*60)
        
        print(f"ğŸ” æ€»æœç´¢æ–‡çŒ®æ•°: {self.stats['total_searched']}")
        print(f"ğŸ“– æˆåŠŸè·å–è¯¦æƒ…: {self.stats['total_fetched']}")
        print(f"âœ… æˆåŠŸç‡: {(self.stats['total_fetched'] / max(self.stats['total_searched'], 1) * 100):.1f}%")
        print(f"âŒ é”™è¯¯æ•°: {self.stats['errors']}")
        
        if self.stats['by_type']:
            print(f"\nğŸ“š æŒ‰æ–‡ç« ç±»å‹åˆ†å¸ƒ:")
            for article_type, count in sorted(self.stats['by_type'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / sum(self.stats['by_type'].values())) * 100
                print(f"  {article_type}: {count}ç¯‡ ({percentage:.1f}%)")
        
        if self.stats['by_year']:
            print(f"\nğŸ“… æŒ‰å‘è¡¨å¹´ä»½åˆ†å¸ƒ:")
            for year, count in sorted(self.stats['by_year'].items(), reverse=True):
                print(f"  {year}: {count}ç¯‡")
        
        print("="*60)

def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸ¥ NEJMä¸“ä¸šæ–‡çŒ®çˆ¬å–å·¥å…·")
    print("="*60)
    print("ä¸“é—¨ç”¨äºçˆ¬å–ã€Šæ–°è‹±æ ¼å…°åŒ»å­¦æ‚å¿—ã€‹çš„é«˜è´¨é‡æ–‡çŒ®")
    print("="*60)
    
    # åˆ›å»ºçˆ¬å–å™¨
    scraper = NEJMLiteratureScraper(email="nejm.research@example.com")
    
    # è®¾ç½®æ—¶é—´èŒƒå›´ï¼ˆè¿‘5å¹´ï¼‰
    end_date = datetime.now()
    start_date = end_date - timedelta(days=5*365)
    
    print(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_date.strftime('%Y/%m/%d')} - {end_date.strftime('%Y/%m/%d')}")
    print(f"ğŸ¯ ç›®æ ‡ç±»å‹: Original Article, Correspondence")
    print(f"ğŸ“§ ä½¿ç”¨é‚®ç®±: {scraper.email}")
    
    # ç¬¬ä¸€æ­¥ï¼šæœç´¢æ–‡çŒ®
    print(f"\nğŸ” ç¬¬ä¸€æ­¥: æœç´¢NEJMæ–‡çŒ®...")
    pmids = scraper.search_nejm_literature(
        start_date=start_date.strftime('%Y/%m/%d'),
        end_date=end_date.strftime('%Y/%m/%d'),
        max_results=2000  # é™åˆ¶æ•°é‡é¿å…è¿‡å¤š
    )
    
    if not pmids:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡çŒ®ï¼Œç¨‹åºç»“æŸ")
        return
    
    # ç¬¬äºŒæ­¥ï¼šè·å–æ–‡çŒ®è¯¦æƒ…
    print(f"\nğŸ“– ç¬¬äºŒæ­¥: è·å–æ–‡çŒ®è¯¦æƒ…...")
    articles = scraper.fetch_literature_details(pmids)
    
    if not articles:
        print("âŒ æœªèƒ½è·å–æ–‡çŒ®è¯¦æƒ…ï¼Œç¨‹åºç»“æŸ")
        return
    
    # ç¬¬ä¸‰æ­¥ï¼šç­›é€‰ç›®æ ‡ç±»å‹
    print(f"\nğŸ¯ ç¬¬ä¸‰æ­¥: ç­›é€‰ç›®æ ‡æ–‡çŒ®ç±»å‹...")
    target_articles = scraper.filter_target_articles(articles)
    
    print(f"âœ… ç­›é€‰å®Œæˆï¼Œå…± {len(target_articles)} ç¯‡ç›®æ ‡æ–‡çŒ®")
    
    # ç¬¬å››æ­¥ï¼šä¿å­˜æ•°æ®
    if target_articles:
        print(f"\nğŸ’¾ ç¬¬å››æ­¥: ä¿å­˜æ•°æ®...")
        saved_files = scraper.save_literature_data(target_articles)
        
        print("âœ… æ•°æ®ä¿å­˜å®Œæˆ:")
        for format_type, file_path in saved_files.items():
            print(f"  ğŸ“„ {format_type.upper()}: {file_path}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        scraper.display_statistics()
        
        # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœé¢„è§ˆ
        print(f"\nğŸ“ ç»“æœé¢„è§ˆï¼ˆå‰5ç¯‡ï¼‰:")
        for i, article in enumerate(target_articles[:5], 1):
            print(f"  {i}. [{article.get('article_type', 'Unknown')}] {article.get('title', 'æ— æ ‡é¢˜')}")
            print(f"     ä½œè€…: {article.get('authors', 'æœªçŸ¥')} | PMID: {article.get('pmid', '')}")
            print(f"     å‘è¡¨: {article.get('pub_date', 'æœªçŸ¥')} | DOI: {article.get('doi', '')}")
            print()
        
        print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±è·å¾— {len(target_articles)} ç¯‡é«˜è´¨é‡NEJMæ–‡çŒ®")
        
    else:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç›®æ ‡æ–‡çŒ®")

if __name__ == "__main__":
    main()